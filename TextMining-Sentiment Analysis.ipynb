{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1hDk9hDvOH7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f8708cb-9059-41f9-f07b-815513c4bd33"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#NLTK-------------------------------\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.snowball import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "#from nltk.stemporter import PorterStemmer\n",
        "\n",
        "# Import libraries for feature \n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#Change current working directory to gdrive\n",
        "%cd /gdrive\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMwGQK7KAd7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e97cef3-3bab-4913-851b-daa5e2f6a284"
      },
      "source": [
        "#Read files\n",
        "textfile = r'/gdrive/My Drive/ASU_MSBA/CIS_508/Assignment 4/Comments.csv'\n",
        "textData = pd.read_csv(textfile) #creates a dataframe\n",
        "\n",
        "CustInfofile = r'/gdrive/My Drive/ASU_MSBA/CIS_508/Assignment 4/Customers.csv'\n",
        "CustInfoData = pd.read_csv(CustInfofile)  #creates a dataframe\n",
        "\n",
        "print(textData.shape)\n",
        "print(CustInfoData.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 2)\n",
            "(2070, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWOTk6C1Ao45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea1a8d6-ea88-4526-f880-b2c5f07753cb"
      },
      "source": [
        "#Extract target column from Customer Info file\n",
        "y = CustInfoData[\"TARGET\"]\n",
        "X_train = CustInfoData.drop(columns=[\"TARGET\"]) #extracting training data without the target column\n",
        "                     \n",
        "print(X_train.shape)\n",
        "print(textData.shape)\n",
        "print(textData.head())\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 16)\n",
            "(2070, 2)\n",
            "     ID                                           Comments\n",
            "0  1309  Does not like the way the phone works. It is t...\n",
            "1  3556  Wanted to know the nearest store location. Wan...\n",
            "2  2230  Wants to know how to do text messaging. Referr...\n",
            "3  2312  Asked how to disable call waiting. referred hi...\n",
            "4  3327  Needs help learning how to use the phone. I su...\n",
            "0       Cancelled\n",
            "1         Current\n",
            "2         Current\n",
            "3         Current\n",
            "4       Cancelled\n",
            "          ...    \n",
            "2065    Cancelled\n",
            "2066    Cancelled\n",
            "2067    Cancelled\n",
            "2068    Cancelled\n",
            "2069    Cancelled\n",
            "Name: TARGET, Length: 2070, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuWYNz2Ep17l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e87707-6dc9-4c91-a6c7-97374f8ff895"
      },
      "source": [
        "#Tokenize - Split the sentences to lists of words\n",
        "textData['CommentsTokenized'] = textData['Comments'].apply(word_tokenize)\n",
        "\n",
        "export_csv = textData.to_csv(r'/gdrive/My Drive/ASU_MSBA/CIS_508/5A/TextDataTokenized1.csv')\n",
        "\n",
        "print(textData.head())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     ID  ...                                  CommentsTokenized\n",
            "0  1309  ...  [Does, not, like, the, way, the, phone, works,...\n",
            "1  3556  ...  [Wanted, to, know, the, nearest, store, locati...\n",
            "2  2230  ...  [Wants, to, know, how, to, do, text, messaging...\n",
            "3  2312  ...  [Asked, how, to, disable, call, waiting, ., re...\n",
            "4  3327  ...  [Needs, help, learning, how, to, use, the, pho...\n",
            "\n",
            "[5 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfuN3lppgeKZ"
      },
      "source": [
        "# Use English stemmer.\n",
        "stemmer2 = PorterStemmer()\n",
        "#Tokenize - Split the sentences to lists of words\n",
        "textData['CommentsTokenized'] = textData['Comments'].apply(word_tokenize)\n",
        "\n",
        "#Now do stemming - create a new dataframe to store stemmed version\n",
        "newTextData=pd.DataFrame()\n",
        "newTextData=textData.drop(columns=[\"CommentsTokenized\",\"Comments\"])\n",
        "newTextData['CommentsTokenizedStemmed'] = textData['CommentsTokenized'].apply(lambda x: [stemmer2.stem(y) for y in x]) # Stem every word.\n",
        "\n",
        "export_csv = newTextData.to_csv(r'/gdrive/My Drive/ASU_MSBA/CIS_508/Assignment 4/newTextDataPS.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeYXLU-u_v9R"
      },
      "source": [
        "# Use English stemmer.\n",
        "stemmer1 = SnowballStemmer(\"english\")\n",
        "#Tokenize - Split the sentences to lists of words\n",
        "textData['CommentsTokenized'] = textData['Comments'].apply(word_tokenize)\n",
        "\n",
        "\n",
        "#Now do stemming - create a new dataframe to store stemmed version\n",
        "newTextData=pd.DataFrame()\n",
        "newTextData=textData.drop(columns=[\"CommentsTokenized\",\"Comments\"])\n",
        "newTextData['CommentsTokenizedStemmed'] = textData['CommentsTokenized'].apply(lambda x: [stemmer1.stem(y) for y in x]) # Stem every word.\n",
        "\n",
        "export_csv = newTextData.to_csv(r'/gdrive/My Drive/ASU_MSBA/CIS_508/Assignment 4/newTextDataSNB.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M07E7VW7_y0d"
      },
      "source": [
        "#Join stemmed strings\n",
        "newTextData['CommentsTokenizedStemmed'] = newTextData['CommentsTokenizedStemmed'].apply(lambda x: \" \".join(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiBguQloljam",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3cb1f21-6c40-4633-c39a-7506daab30b7"
      },
      "source": [
        "#Do Bag-Of-Words model - Term - Document Matrix\n",
        "#Learn the vocabulary dictionary and return term-document matrix.\n",
        "#count_vect = CountVectorizer(stop_words=None)\n",
        "count_vect = CountVectorizer(stop_words='english',lowercase=False)\n",
        "TD_counts = count_vect.fit_transform(newTextData.CommentsTokenizedStemmed)\n",
        "print(TD_counts.shape)\n",
        "print(TD_counts.dtype)\n",
        "print(count_vect.get_feature_names())\n",
        "#print(TD_counts)\n",
        "DF_TD_Counts=pd.DataFrame(TD_counts.toarray())\n",
        "print(DF_TD_Counts)\n",
        "export_csv = DF_TD_Counts.to_csv(r'/gdrive/My Drive/ASU_MSBA/CIS_508/Assignment 4//TD_counts-TokenizedStemmed.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 354)\n",
            "int64\n",
            "['3399', '3g', 'abysm', 'access', 'accessori', 'adapt', 'add', 'addit', 'additon', 'address', 'adit', 'adress', 'advertis', 'afraid', 'alway', 'angel', 'angri', 'ani', 'anoth', 'anyth', 'anytim', 'area', 'asap', 'ask', 'bad', 'basic', 'bateri', 'batteri', 'becaus', 'believ', 'better', 'bigger', 'book', 'bought', 'brain', 'bring', 'built', 'busi', 'button', 'buy', 'cancel', 'cancer', 'car', 'care', 'carrier', 'caus', 'cc', 'cell', 'certain', 'chang', 'charg', 'charger', 'check', 'chip', 'citi', 'claim', 'cleariti', 'cold', 'comapr', 'compani', 'compar', 'competit', 'complain', 'complaint', 'concept', 'connect', 'consisit', 'consist', 'constan', 'contact', 'continu', 'contract', 'correct', 'cost', 'coupl', 'cover', 'coverag', 'creat', 'credit', 'cstmer', 'cstmr', 'current', 'cust', 'custom', 'customr', 'date', 'day', 'dead', 'decent', 'defect', 'deo', 'did', 'die', 'differ', 'difficult', 'digiti', 'direct', 'disabl', 'doe', 'don', 'dont', 'drop', 'dure', 'easier', 'effect', 'encount', 'end', 'enemi', 'equip', 'everytim', 'everywher', 'evrey', 'exact', 'expect', 'expir', 'explain', 'facepl', 'fals', 'famili', 'featur', 'fed', 'figur', 'fine', 'fix', 'forev', 'forward', 'friend', 'function', 'furthermor', 'futur', 'gave', 'goat', 'good', 'great', 'gsm', 'handset', 'happi', 'hard', 'hate', 'hear', 'heard', 'help', 'higher', 'highway', 'hochi', 'hole', 'home', 'hope', 'horribl', 'hous', 'implement', 'improv', 'inadequ', 'includ', 'info', 'inform', 'ing', 'internet', 'intersect', 'issu', 'june', 'just', 'kid', 'kno', 'know', 'lame', 'later', 'lctn', 'learn', 'leroy', 'like', 'line', 'list', 'local', 'locat', 'locatn', 'long', 'los', 'lost', 'lot', 'love', 'major', 'make', 'manag', 'mani', 'manual', 'market', 'mean', 'messag', 'metropolitian', 'minut', 'misl', 'mistak', 'model', 'momma', 'mr', 'napeleon', 'near', 'nearest', 'need', 'network', 'new', 'news', 'notic', 'number', 'numer', 'offer', 'old', 'om', 'open', 'option', 'ori', 'ot', 'outbound', 'pass', 'pay', 'pda', 'peopl', 'perform', 'person', 'phone', 'piec', 'plan', 'pleas', 'point', 'polici', 'poor', 'possibl', 'probabl', 'problem', 'proper', 'provid', 'provis', 'purpos', 'rate', 'rater', 'realiz', 'realli', 'reason', 'receiv', 'recept', 'recption', 'reenter', 'refer', 'relat', 'rep', 'replac', 'respect', 'result', 'rid', 'right', 'ring', 'roam', 'roll', 'rubbish', 'rude', 'said', 'sale', 'say', 'screen', 'self', 'send', 'servic', 'shitti', 'shut', 'sign', 'signal', 'signific', 'simm', 'simpli', 'sinc', 'site', 'slow', 'sold', 'someon', 'sometim', 'soon', 'speak', 'speed', 'start', 'static', 'stole', 'store', 'stuff', 'stupid', 'substant', 'subtract', 'suck', 'suggest', 'supervisor', 'support', 'sure', 'surpris', 'suspect', 'suspend', 'switch', 'teach', 'technic', 'tell', 'terribl', 'test', 'text', 'think', 'thought', 'ticket', 'till', 'time', 'tire', 'today', 'toilet', 'told', 'tone', 'tower', 'transeff', 'transf', 'transfer', 'travel', 'tri', 'trust', 'turn', 'uncomfort', 'understand', 'unhappi', 'unlimit', 'unreli', 'unwil', 'upset', 'usag', 'use', 'useless', 'valu', 'veri', 'vm', 'wa', 'wait', 'want', 'wast', 'way', 'weak', 'web', 'websit', 'week', 'whi', 'wife', 'wish', 'wll', 'wold', 'work', 'wors', 'worst', 'wrong', 'xvyx', 'year', 'york']\n",
            "      0    1    2    3    4    5    6    ...  347  348  349  350  351  352  353\n",
            "0       0    0    0    0    0    0    0  ...    1    0    0    0    0    0    0\n",
            "1       0    0    0    0    1    0    0  ...    0    0    0    0    0    0    0\n",
            "2       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "3       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "4       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "2065    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2066    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2067    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2068    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "2069    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
            "\n",
            "[2070 rows x 354 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd8TZYnAxQbP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61904ab0-c3e9-4362-f3f3-bdb5d2a1d6d7"
      },
      "source": [
        "#Compute TF-IDF Matrix\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(TD_counts)\n",
        "print(X_train_tfidf.shape)\n",
        "DF_TF_IDF=pd.DataFrame(X_train_tfidf.toarray())\n",
        "print(DF_TF_IDF)\n",
        "export_csv= DF_TF_IDF.to_csv(r'/gdrive/My Drive/ASU_MSBA/CIS_508/Assignment 4/TFIDF_counts-TokenizedStemmed.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 354)\n",
            "      0    1    2    3        4    5    ...  348  349  350  351  352  353\n",
            "0     0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "1     0.0  0.0  0.0  0.0  0.27568  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2     0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "3     0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "4     0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "...   ...  ...  ...  ...      ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "2065  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2066  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2067  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2068  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2069  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[2070 rows x 354 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiUqTc6T5sCw"
      },
      "source": [
        "id= textData.iloc[:,0]\n",
        "combined=pd.concat([id,DF_TF_IDF], axis=1)\n",
        "combined1=X_train.merge(combined, on=['ID'])\n",
        "DF_TF_IDF=combined1.drop(columns=['ID'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRlQ18KyxAK4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "a8e1ac3e-a003-4c2d-ec50-c8a9fdb98465"
      },
      "source": [
        "# One Hot Encoding\n",
        "#The below function returns a list of categorical features which are not numeric. \n",
        "cat_cols = DF_TF_IDF.select_dtypes(exclude=['float','int']).columns #selecting the categorical columns\n",
        "print(cat_cols.shape)\n",
        "print(cat_cols)\n",
        "\n",
        "# OneHotEncoding is to be done on Categorical variables.\n",
        "ohe = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
        "Xcat = pd.DataFrame(ohe.fit_transform(DF_TF_IDF[cat_cols]),columns=ohe.get_feature_names(),index=DF_TF_IDF.index)\n",
        "DF_TF_IDF = pd.concat([DF_TF_IDF,Xcat],axis=1)\n",
        "DF_TF_IDF.drop(labels=cat_cols,axis=1,inplace=True)\n",
        "DF_TF_IDF.sample(5)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6,)\n",
            "Index(['Sex', 'Status', 'Car_Owner', 'Paymethod', 'LocalBilltype',\n",
            "       'LongDistanceBilltype'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Children</th>\n",
              "      <th>Est_Income</th>\n",
              "      <th>Usage</th>\n",
              "      <th>Age</th>\n",
              "      <th>RatePlan</th>\n",
              "      <th>LongDistance</th>\n",
              "      <th>International</th>\n",
              "      <th>Local</th>\n",
              "      <th>Dropped</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>...</th>\n",
              "      <th>328</th>\n",
              "      <th>329</th>\n",
              "      <th>330</th>\n",
              "      <th>331</th>\n",
              "      <th>332</th>\n",
              "      <th>333</th>\n",
              "      <th>334</th>\n",
              "      <th>335</th>\n",
              "      <th>336</th>\n",
              "      <th>337</th>\n",
              "      <th>338</th>\n",
              "      <th>339</th>\n",
              "      <th>340</th>\n",
              "      <th>341</th>\n",
              "      <th>342</th>\n",
              "      <th>343</th>\n",
              "      <th>344</th>\n",
              "      <th>345</th>\n",
              "      <th>346</th>\n",
              "      <th>347</th>\n",
              "      <th>348</th>\n",
              "      <th>349</th>\n",
              "      <th>350</th>\n",
              "      <th>351</th>\n",
              "      <th>352</th>\n",
              "      <th>353</th>\n",
              "      <th>x0_F</th>\n",
              "      <th>x0_M</th>\n",
              "      <th>x1_D</th>\n",
              "      <th>x1_M</th>\n",
              "      <th>x1_S</th>\n",
              "      <th>x2_N</th>\n",
              "      <th>x2_Y</th>\n",
              "      <th>x3_Auto</th>\n",
              "      <th>x3_CC</th>\n",
              "      <th>x3_CH</th>\n",
              "      <th>x4_Budget</th>\n",
              "      <th>x4_FreeLocal</th>\n",
              "      <th>x5_Intnl_discount</th>\n",
              "      <th>x5_Standard</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>522</th>\n",
              "      <td>0</td>\n",
              "      <td>96681.0</td>\n",
              "      <td>73.76</td>\n",
              "      <td>19.760000</td>\n",
              "      <td>2</td>\n",
              "      <td>2.00</td>\n",
              "      <td>7.66</td>\n",
              "      <td>40.92</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.772949</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.324251</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1234</th>\n",
              "      <td>2</td>\n",
              "      <td>64816.6</td>\n",
              "      <td>27.87</td>\n",
              "      <td>31.126667</td>\n",
              "      <td>4</td>\n",
              "      <td>21.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>6.87</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.204572</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1062</th>\n",
              "      <td>2</td>\n",
              "      <td>36725.1</td>\n",
              "      <td>94.88</td>\n",
              "      <td>55.793333</td>\n",
              "      <td>4</td>\n",
              "      <td>8.65</td>\n",
              "      <td>9.22</td>\n",
              "      <td>77.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.293881</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1244</th>\n",
              "      <td>0</td>\n",
              "      <td>64319.2</td>\n",
              "      <td>40.16</td>\n",
              "      <td>41.226667</td>\n",
              "      <td>1</td>\n",
              "      <td>3.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>37.15</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1679</th>\n",
              "      <td>0</td>\n",
              "      <td>95405.7</td>\n",
              "      <td>13.96</td>\n",
              "      <td>46.506667</td>\n",
              "      <td>4</td>\n",
              "      <td>7.74</td>\n",
              "      <td>0.00</td>\n",
              "      <td>6.21</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.368149</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.334347</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.416738</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 377 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Children  Est_Income  Usage  ...  x4_FreeLocal  x5_Intnl_discount  x5_Standard\n",
              "522          0     96681.0  73.76  ...           0.0                0.0          1.0\n",
              "1234         2     64816.6  27.87  ...           0.0                0.0          1.0\n",
              "1062         2     36725.1  94.88  ...           0.0                0.0          1.0\n",
              "1244         0     64319.2  40.16  ...           0.0                1.0          0.0\n",
              "1679         0     95405.7  13.96  ...           0.0                0.0          1.0\n",
              "\n",
              "[5 rows x 377 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjrDtxSSzoEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96505925-c267-4196-a95d-595e17c16986"
      },
      "source": [
        "#Feature selection Filter Method Chi square\n",
        "new_DF_TF_IDF = SelectKBest(score_func=chi2, k=50).fit_transform(DF_TF_IDF,y)\n",
        "print(new_DF_TF_IDF.shape)\n",
        "\n",
        "DF_TF_IDF_SelectedFeatures= pd.DataFrame(new_DF_TF_IDF)\n",
        "print(DF_TF_IDF_SelectedFeatures)\n",
        "\n",
        "#Construct a Random Forest Classifier on text data\n",
        "clf=RandomForestClassifier()\n",
        "RF_text = clf.fit(DF_TF_IDF_SelectedFeatures,y)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(DF_TF_IDF_SelectedFeatures,y)))\n",
        "rf_predictions = clf.predict(DF_TF_IDF_SelectedFeatures)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y, rf_predictions))\n",
        "\n",
        "#run cross-validation - Text Data\n",
        "clf_cv_score = cross_val_score(clf, DF_TF_IDF_SelectedFeatures, y, cv=20, scoring=\"balanced_accuracy\")\n",
        "print(\"=== All Accuracy Scores ===\")\n",
        "print(clf_cv_score)\n",
        "print('\\n')\n",
        "print(\"=== Mean Accuracy Score ===\")\n",
        "print(\"Mean Accuracy Score - ON Text: \",clf_cv_score.mean())\n",
        "print('\\n')\n",
        "DF_TF_IDF_Filter50=DF_TF_IDF_SelectedFeatures"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 50)\n",
            "       0         1       2      3     4   ...   45   46   47   48   49\n",
            "0     1.0  38000.00  229.64  23.56  0.00  ...  1.0  0.0  0.0  1.0  0.0\n",
            "1     2.0  29616.00   75.29  29.78  0.00  ...  0.0  1.0  1.0  0.0  0.0\n",
            "2     0.0  19732.80   47.25  24.81  0.00  ...  0.0  1.0  1.0  0.0  0.0\n",
            "3     2.0     96.33   59.01  26.13  0.00  ...  0.0  1.0  0.0  1.0  0.0\n",
            "4     2.0  52004.80   28.14   5.03  0.00  ...  1.0  0.0  1.0  0.0  0.0\n",
            "...   ...       ...     ...    ...   ...  ...  ...  ...  ...  ...  ...\n",
            "2065  0.0  78851.30   29.04   0.37  0.00  ...  1.0  0.0  0.0  1.0  0.0\n",
            "2066  1.0  17540.70   36.20  22.17  0.57  ...  1.0  0.0  0.0  1.0  1.0\n",
            "2067  0.0  83891.90   74.40  28.92  0.00  ...  1.0  0.0  1.0  0.0  0.0\n",
            "2068  2.0  28220.80   38.95  26.49  0.00  ...  1.0  0.0  1.0  0.0  0.0\n",
            "2069  0.0  28589.10  100.28  13.19  0.00  ...  1.0  0.0  0.0  1.0  0.0\n",
            "\n",
            "[2070 rows x 50 columns]\n",
            "Accuracy score (training): 0.926570\n",
            "Confusion Matrix:\n",
            "[[ 729   75]\n",
            " [  77 1189]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.90      0.91      0.91       804\n",
            "     Current       0.94      0.94      0.94      1266\n",
            "\n",
            "    accuracy                           0.93      2070\n",
            "   macro avg       0.92      0.92      0.92      2070\n",
            "weighted avg       0.93      0.93      0.93      2070\n",
            "\n",
            "=== All Accuracy Scores ===\n",
            "[0.82791328 0.86391792 0.90301974 0.89566396 0.825      0.93125\n",
            " 0.8265625  0.809375   0.8671875  0.89375    0.88869048 0.86825397\n",
            " 0.93412698 0.76507937 0.90575397 0.84781746 0.91031746 0.93869048\n",
            " 0.90238095 0.93531746]\n",
            "\n",
            "\n",
            "=== Mean Accuracy Score ===\n",
            "Mean Accuracy Score - ON Text:  0.8770034238288812\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPedDXfc0Ny7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef36651-959d-4b58-f82c-d620403a5ae1"
      },
      "source": [
        "#Feature selection Filter Method Chi square\n",
        "new_DF_TF_IDF = SelectKBest(score_func=chi2, k=10).fit_transform(DF_TF_IDF,y)\n",
        "print(new_DF_TF_IDF.shape)\n",
        "\n",
        "DF_TF_IDF_SelectedFeatures= pd.DataFrame(new_DF_TF_IDF)\n",
        "print(DF_TF_IDF_SelectedFeatures)\n",
        "\n",
        "\n",
        "#Construct a Random Forest Classifier on text data\n",
        "clf=RandomForestClassifier()\n",
        "RF_text = clf.fit(DF_TF_IDF_SelectedFeatures,y)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(DF_TF_IDF_SelectedFeatures,y)))\n",
        "rf_predictions = clf.predict(DF_TF_IDF_SelectedFeatures)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y, rf_predictions))\n",
        "\n",
        "#run cross-validation - Text Data\n",
        "clf_cv_score = cross_val_score(clf, DF_TF_IDF_SelectedFeatures, y, cv=20, scoring=\"balanced_accuracy\")\n",
        "print(\"=== All Accuracy Scores ===\")\n",
        "print(clf_cv_score)\n",
        "print('\\n')\n",
        "print(\"=== Mean Accuracy Score ===\")\n",
        "print(\"Mean Accuracy Score - ON Text: \",clf_cv_score.mean())\n",
        "print('\\n')\n",
        "DF_TF_IDF_Filter10=DF_TF_IDF_SelectedFeatures"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 10)\n",
            "        0         1       2      3     4       5    6    7    8    9\n",
            "0     1.0  38000.00  229.64  23.56  0.00  206.08  0.0  0.0  0.0  1.0\n",
            "1     2.0  29616.00   75.29  29.78  0.00   45.50  0.0  0.0  1.0  0.0\n",
            "2     0.0  19732.80   47.25  24.81  0.00   22.44  0.0  0.0  1.0  0.0\n",
            "3     2.0     96.33   59.01  26.13  0.00   32.88  0.0  0.0  0.0  1.0\n",
            "4     2.0  52004.80   28.14   5.03  0.00   23.11  0.0  0.0  1.0  0.0\n",
            "...   ...       ...     ...    ...   ...     ...  ...  ...  ...  ...\n",
            "2065  0.0  78851.30   29.04   0.37  0.00   28.66  0.0  0.0  0.0  1.0\n",
            "2066  1.0  17540.70   36.20  22.17  0.57   13.45  0.0  0.0  0.0  1.0\n",
            "2067  0.0  83891.90   74.40  28.92  0.00   45.47  0.0  0.0  1.0  0.0\n",
            "2068  2.0  28220.80   38.95  26.49  0.00   12.46  0.0  0.0  1.0  0.0\n",
            "2069  0.0  28589.10  100.28  13.19  0.00   87.09  0.0  0.0  0.0  1.0\n",
            "\n",
            "[2070 rows x 10 columns]\n",
            "Accuracy score (training): 0.917874\n",
            "Confusion Matrix:\n",
            "[[ 719   85]\n",
            " [  85 1181]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.89      0.89      0.89       804\n",
            "     Current       0.93      0.93      0.93      1266\n",
            "\n",
            "    accuracy                           0.92      2070\n",
            "   macro avg       0.91      0.91      0.91      2070\n",
            "weighted avg       0.92      0.92      0.92      2070\n",
            "\n",
            "=== All Accuracy Scores ===\n",
            "[0.85230352 0.86391792 0.90727836 0.88772745 0.825      0.91875\n",
            " 0.815625   0.8109375  0.8546875  0.88125    0.88531746 0.86031746\n",
            " 0.93412698 0.7968254  0.86825397 0.82281746 0.88988095 0.87738095\n",
            " 0.84900794 0.90575397]\n",
            "\n",
            "\n",
            "=== Mean Accuracy Score ===\n",
            "Mean Accuracy Score - ON Text:  0.8653579897406118\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gsyh0gY0WFv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d217f22-8efb-4c5f-cc13-412d4e467d50"
      },
      "source": [
        "#Feature selection Filter Method\n",
        "new_DF_TF_IDF = SelectKBest(score_func=chi2, k=15).fit_transform(DF_TF_IDF,y)\n",
        "print(new_DF_TF_IDF.shape)\n",
        "\n",
        "DF_TF_IDF_SelectedFeatures= pd.DataFrame(new_DF_TF_IDF)\n",
        "print(DF_TF_IDF_SelectedFeatures)\n",
        "\n",
        "\n",
        "#Construct a Random Forest Classifier on text data\n",
        "clf=RandomForestClassifier()\n",
        "RF_text = clf.fit(DF_TF_IDF_SelectedFeatures,y)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(DF_TF_IDF_SelectedFeatures,y)))\n",
        "rf_predictions = clf.predict(DF_TF_IDF_SelectedFeatures)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y, rf_predictions))\n",
        "\n",
        "#run cross-validation - Text Data\n",
        "clf_cv_score = cross_val_score(clf, DF_TF_IDF_SelectedFeatures, y, cv=20, scoring=\"balanced_accuracy\")\n",
        "print(\"=== All Accuracy Scores ===\")\n",
        "print(clf_cv_score)\n",
        "print('\\n')\n",
        "print(\"=== Mean Accuracy Score ===\")\n",
        "print(\"Mean Accuracy Score - ON Text: \",clf_cv_score.mean())\n",
        "print('\\n')\n",
        "DF_TF_IDF_Filter15=DF_TF_IDF_SelectedFeatures"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 15)\n",
            "       0         1       2      3     4   ...       10   11   12   13   14\n",
            "0     1.0  38000.00  229.64  23.56  0.00  ...  0.00000  0.0  0.0  0.0  1.0\n",
            "1     2.0  29616.00   75.29  29.78  0.00  ...  0.37653  0.0  1.0  1.0  0.0\n",
            "2     0.0  19732.80   47.25  24.81  0.00  ...  0.37653  0.0  1.0  1.0  0.0\n",
            "3     2.0     96.33   59.01  26.13  0.00  ...  0.37653  0.0  1.0  0.0  1.0\n",
            "4     2.0  52004.80   28.14   5.03  0.00  ...  0.37653  0.0  0.0  1.0  0.0\n",
            "...   ...       ...     ...    ...   ...  ...      ...  ...  ...  ...  ...\n",
            "2065  0.0  78851.30   29.04   0.37  0.00  ...  0.00000  0.0  0.0  0.0  1.0\n",
            "2066  1.0  17540.70   36.20  22.17  0.57  ...  0.00000  0.0  0.0  0.0  1.0\n",
            "2067  0.0  83891.90   74.40  28.92  0.00  ...  0.00000  0.0  0.0  1.0  0.0\n",
            "2068  2.0  28220.80   38.95  26.49  0.00  ...  0.00000  0.0  0.0  1.0  0.0\n",
            "2069  0.0  28589.10  100.28  13.19  0.00  ...  0.00000  0.0  0.0  0.0  1.0\n",
            "\n",
            "[2070 rows x 15 columns]\n",
            "Accuracy score (training): 0.923188\n",
            "Confusion Matrix:\n",
            "[[ 730   74]\n",
            " [  85 1181]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.90      0.91      0.90       804\n",
            "     Current       0.94      0.93      0.94      1266\n",
            "\n",
            "    accuracy                           0.92      2070\n",
            "   macro avg       0.92      0.92      0.92      2070\n",
            "weighted avg       0.92      0.92      0.92      2070\n",
            "\n",
            "=== All Accuracy Scores ===\n",
            "[0.8401084  0.86391792 0.89508324 0.88346883 0.8125     0.9109375\n",
            " 0.8265625  0.8015625  0.8296875  0.89375    0.91369048 0.88075397\n",
            " 0.93412698 0.7968254  0.89325397 0.82281746 0.90238095 0.92281746\n",
            " 0.89444444 0.91031746]\n",
            "\n",
            "\n",
            "=== Mean Accuracy Score ===\n",
            "Mean Accuracy Score - ON Text:  0.8714503484320557\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx7btFngTVN7"
      },
      "source": [
        "# Feature selection Filter Method Chi square and k=50\n",
        "# Mean Accuracy Score - ON Text:  0.879707311024003\n",
        "#Feature selection Filter Method Chi square and k=10\n",
        "#Mean Accuracy Score - ON Text:  0.862332559039876\n",
        "#Feature selection Filter Method Chi square and k=15\n",
        "#Mean Accuracy Score - ON Text:  0.8737007295296169"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq_7eeQOz7xk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a47e4851-8929-4072-d7a5-c4f252754599"
      },
      "source": [
        "# Split the data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(DF_TF_IDF_Filter50, y, test_size = 0.2, random_state = 1)\n",
        "\n",
        "#Construct a Random Forest Classifier on combined data\n",
        "clf=RandomForestClassifier()\n",
        "RF_Comb = clf.fit(X_train,y_train)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(X_train, y_train)))\n",
        "rf_predictions = clf.predict(X_test)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_test, rf_predictions))\n",
        "\n",
        "#run cross-validation - COMBINED Data\n",
        "rf_Comb_cv_score = cross_val_score(RF_Comb, DF_TF_IDF_Filter50, y, cv=20, scoring=\"balanced_accuracy\")\n",
        "print(\"=== All Accuracy Scores ===\")\n",
        "print(rf_Comb_cv_score)\n",
        "print('\\n')\n",
        "print(\"=== Mean Accuracy Score ===\")\n",
        "print(\"Mean Accuracy Score - ON Text: \",rf_Comb_cv_score.mean())\n",
        "print('\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score (training): 0.930556\n",
            "Confusion Matrix:\n",
            "[[128  22]\n",
            " [ 30 234]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.81      0.85      0.83       150\n",
            "     Current       0.91      0.89      0.90       264\n",
            "\n",
            "    accuracy                           0.87       414\n",
            "   macro avg       0.86      0.87      0.87       414\n",
            "weighted avg       0.88      0.87      0.88       414\n",
            "\n",
            "=== All Accuracy Scores ===\n",
            "[0.82791328 0.86391792 0.90301974 0.89566396 0.825      0.9015625\n",
            " 0.8265625  0.809375   0.8296875  0.89375    0.88075397 0.89325397\n",
            " 0.93412698 0.76507937 0.90575397 0.86031746 0.90238095 0.93869048\n",
            " 0.89444444 0.91825397]\n",
            "\n",
            "\n",
            "=== Mean Accuracy Score ===\n",
            "Mean Accuracy Score - ON Text:  0.8734753980352303\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCturU8ElWAD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b1d0c16-2460-4b17-c964-8a03457ef906"
      },
      "source": [
        "#Feature selection Filter Method ANOVA\n",
        "new_DF_TF_IDF = SelectKBest(score_func=f_classif, k=50).fit_transform(DF_TF_IDF,y)\n",
        "print(new_DF_TF_IDF.shape)\n",
        "\n",
        "DF_TF_IDF_SelectedFeatures= pd.DataFrame(new_DF_TF_IDF)\n",
        "print(DF_TF_IDF_SelectedFeatures)\n",
        "\n",
        "#Construct a Random Forest Classifier on text data\n",
        "clf=RandomForestClassifier()\n",
        "RF_text = clf.fit(DF_TF_IDF_SelectedFeatures,y)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(DF_TF_IDF_SelectedFeatures,y)))\n",
        "rf_predictions = clf.predict(DF_TF_IDF_SelectedFeatures)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y, rf_predictions))\n",
        "\n",
        "#run cross-validation - Text Data\n",
        "clf_cv_score = cross_val_score(clf, DF_TF_IDF_SelectedFeatures, y, cv=20, scoring=\"balanced_accuracy\")\n",
        "print(\"=== All Accuracy Scores ===\")\n",
        "print(clf_cv_score)\n",
        "print('\\n')\n",
        "print(\"=== Mean Accuracy Score ===\")\n",
        "print(\"Mean Accuracy Score - ON Text: \",clf_cv_score.mean())\n",
        "print('\\n')\n",
        "DF_TF_IDF_Filter50=DF_TF_IDF_SelectedFeatures"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 50)\n",
            "       0         1       2      3     4   ...   45   46   47   48   49\n",
            "0     1.0  38000.00  229.64  23.56  0.00  ...  1.0  0.0  0.0  1.0  0.0\n",
            "1     2.0  29616.00   75.29  29.78  0.00  ...  0.0  1.0  1.0  0.0  0.0\n",
            "2     0.0  19732.80   47.25  24.81  0.00  ...  0.0  1.0  1.0  0.0  0.0\n",
            "3     2.0     96.33   59.01  26.13  0.00  ...  0.0  1.0  0.0  1.0  0.0\n",
            "4     2.0  52004.80   28.14   5.03  0.00  ...  1.0  0.0  1.0  0.0  0.0\n",
            "...   ...       ...     ...    ...   ...  ...  ...  ...  ...  ...  ...\n",
            "2065  0.0  78851.30   29.04   0.37  0.00  ...  1.0  0.0  0.0  1.0  0.0\n",
            "2066  1.0  17540.70   36.20  22.17  0.57  ...  1.0  0.0  0.0  1.0  1.0\n",
            "2067  0.0  83891.90   74.40  28.92  0.00  ...  1.0  0.0  1.0  0.0  0.0\n",
            "2068  2.0  28220.80   38.95  26.49  0.00  ...  1.0  0.0  1.0  0.0  0.0\n",
            "2069  0.0  28589.10  100.28  13.19  0.00  ...  1.0  0.0  0.0  1.0  0.0\n",
            "\n",
            "[2070 rows x 50 columns]\n",
            "Accuracy score (training): 0.926570\n",
            "Confusion Matrix:\n",
            "[[ 741   63]\n",
            " [  89 1177]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.89      0.92      0.91       804\n",
            "     Current       0.95      0.93      0.94      1266\n",
            "\n",
            "    accuracy                           0.93      2070\n",
            "   macro avg       0.92      0.93      0.92      2070\n",
            "weighted avg       0.93      0.93      0.93      2070\n",
            "\n",
            "=== All Accuracy Scores ===\n",
            "[0.83584979 0.86391792 0.90301974 0.89566396 0.825      0.9390625\n",
            " 0.8265625  0.809375   0.8796875  0.89375    0.89325397 0.89325397\n",
            " 0.93412698 0.77301587 0.89325397 0.82281746 0.90238095 0.93075397\n",
            " 0.90238095 0.91031746]\n",
            "\n",
            "\n",
            "=== Mean Accuracy Score ===\n",
            "Mean Accuracy Score - ON Text:  0.8763722234320559\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP25XhfG3KeB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c386234-3ada-4e43-b46d-43be81a2e9b2"
      },
      "source": [
        "#Feature selection Filter Method ANOVA\n",
        "new_DF_TF_IDF = SelectKBest(score_func=f_classif, k=10).fit_transform(DF_TF_IDF,y)\n",
        "print(new_DF_TF_IDF.shape)\n",
        "\n",
        "DF_TF_IDF_SelectedFeatures= pd.DataFrame(new_DF_TF_IDF)\n",
        "print(DF_TF_IDF_SelectedFeatures)\n",
        "\n",
        "\n",
        "#Construct a Random Forest Classifier on text data\n",
        "clf=RandomForestClassifier()\n",
        "RF_text = clf.fit(DF_TF_IDF_SelectedFeatures,y)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(DF_TF_IDF_SelectedFeatures,y)))\n",
        "rf_predictions = clf.predict(DF_TF_IDF_SelectedFeatures)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y, rf_predictions))\n",
        "\n",
        "#run cross-validation - Text Data\n",
        "clf_cv_score = cross_val_score(clf, DF_TF_IDF_SelectedFeatures, y, cv=20, scoring=\"balanced_accuracy\")\n",
        "print(\"=== All Accuracy Scores ===\")\n",
        "print(clf_cv_score)\n",
        "print('\\n')\n",
        "print(\"=== Mean Accuracy Score ===\")\n",
        "print(\"Mean Accuracy Score - ON Text: \",clf_cv_score.mean())\n",
        "print('\\n')\n",
        "DF_TF_IDF_Filter10=DF_TF_IDF_SelectedFeatures"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 10)\n",
            "        0        1        2         3    4        5    6         7    8    9\n",
            "0     1.0  0.00000  0.00000  0.000000  0.0  0.00000  0.0  0.000000  0.0  1.0\n",
            "1     2.0  0.37653  0.37653  0.271105  0.0  0.37653  0.0  0.348322  1.0  0.0\n",
            "2     0.0  0.37653  0.37653  0.271105  0.0  0.37653  0.0  0.348322  1.0  0.0\n",
            "3     2.0  0.37653  0.37653  0.271105  0.0  0.37653  0.0  0.348322  0.0  1.0\n",
            "4     2.0  0.37653  0.37653  0.271105  0.0  0.37653  0.0  0.348322  1.0  0.0\n",
            "...   ...      ...      ...       ...  ...      ...  ...       ...  ...  ...\n",
            "2065  0.0  0.00000  0.00000  0.000000  0.0  0.00000  0.0  0.000000  0.0  1.0\n",
            "2066  1.0  0.00000  0.00000  0.000000  0.0  0.00000  0.0  0.000000  0.0  1.0\n",
            "2067  0.0  0.00000  0.00000  0.000000  0.0  0.00000  0.0  0.000000  1.0  0.0\n",
            "2068  2.0  0.00000  0.00000  0.000000  0.0  0.00000  0.0  0.000000  1.0  0.0\n",
            "2069  0.0  0.00000  0.00000  0.000000  0.0  0.00000  0.0  0.000000  0.0  1.0\n",
            "\n",
            "[2070 rows x 10 columns]\n",
            "Accuracy score (training): 0.784541\n",
            "Confusion Matrix:\n",
            "[[703 101]\n",
            " [345 921]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.67      0.87      0.76       804\n",
            "     Current       0.90      0.73      0.81      1266\n",
            "\n",
            "    accuracy                           0.78      2070\n",
            "   macro avg       0.79      0.80      0.78      2070\n",
            "weighted avg       0.81      0.78      0.79      2070\n",
            "\n",
            "=== All Accuracy Scores ===\n",
            "[0.78881146 0.78881146 0.82791328 0.82055749 0.7578125  0.76875\n",
            " 0.7984375  0.715625   0.7671875  0.790625   0.81964286 0.74246032\n",
            " 0.87400794 0.7265873  0.80595238 0.86845238 0.8343254  0.83313492\n",
            " 0.7968254  0.83888889]\n",
            "\n",
            "\n",
            "=== Mean Accuracy Score ===\n",
            "Mean Accuracy Score - ON Text:  0.7982404483643052\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AARbvI5P3cyS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c6b349-cd1e-4b8b-bfd1-24bb918519bf"
      },
      "source": [
        "#Feature selection Filter Method ANOVA\n",
        "new_DF_TF_IDF = SelectKBest(score_func=f_classif, k=15).fit_transform(DF_TF_IDF,y)\n",
        "print(new_DF_TF_IDF.shape)\n",
        "\n",
        "DF_TF_IDF_SelectedFeatures= pd.DataFrame(new_DF_TF_IDF)\n",
        "print(DF_TF_IDF_SelectedFeatures)\n",
        "\n",
        "\n",
        "#Construct a Random Forest Classifier on text data\n",
        "clf=RandomForestClassifier()\n",
        "RF_text = clf.fit(DF_TF_IDF_SelectedFeatures,y)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(DF_TF_IDF_SelectedFeatures,y)))\n",
        "rf_predictions = clf.predict(DF_TF_IDF_SelectedFeatures)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y, rf_predictions))\n",
        "\n",
        "#run cross-validation - Text Data\n",
        "clf_cv_score = cross_val_score(clf, DF_TF_IDF_SelectedFeatures, y, cv=20, scoring=\"balanced_accuracy\")\n",
        "print(\"=== All Accuracy Scores ===\")\n",
        "print(clf_cv_score)\n",
        "print('\\n')\n",
        "print(\"=== Mean Accuracy Score ===\")\n",
        "print(\"Mean Accuracy Score - ON Text: \",clf_cv_score.mean())\n",
        "print('\\n')\n",
        "DF_TF_IDF_Filter15=DF_TF_IDF_SelectedFeatures"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2070, 15)\n",
            "       0    1    2        3    4    5   ...   9        10   11        12   13   14\n",
            "0     1.0  0.0  0.0  0.00000  0.0  0.0  ...  0.0  0.00000  0.0  0.000000  0.0  1.0\n",
            "1     2.0  0.0  0.0  0.37653  0.0  0.0  ...  0.0  0.37653  0.0  0.348322  1.0  0.0\n",
            "2     0.0  0.0  0.0  0.37653  0.0  0.0  ...  0.0  0.37653  0.0  0.348322  1.0  0.0\n",
            "3     2.0  0.0  0.0  0.37653  0.0  0.0  ...  0.0  0.37653  0.0  0.348322  0.0  1.0\n",
            "4     2.0  0.0  0.0  0.37653  0.0  0.0  ...  0.0  0.37653  0.0  0.348322  1.0  0.0\n",
            "...   ...  ...  ...      ...  ...  ...  ...  ...      ...  ...       ...  ...  ...\n",
            "2065  0.0  0.0  0.0  0.00000  0.0  0.0  ...  0.0  0.00000  0.0  0.000000  0.0  1.0\n",
            "2066  1.0  0.0  0.0  0.00000  0.0  0.0  ...  0.0  0.00000  0.0  0.000000  0.0  1.0\n",
            "2067  0.0  0.0  0.0  0.00000  0.0  0.0  ...  0.0  0.00000  0.0  0.000000  1.0  0.0\n",
            "2068  2.0  0.0  0.0  0.00000  0.0  0.0  ...  0.0  0.00000  0.0  0.000000  1.0  0.0\n",
            "2069  0.0  0.0  0.0  0.00000  0.0  0.0  ...  0.0  0.00000  0.0  0.000000  0.0  1.0\n",
            "\n",
            "[2070 rows x 15 columns]\n",
            "Accuracy score (training): 0.784541\n",
            "Confusion Matrix:\n",
            "[[703 101]\n",
            " [345 921]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.67      0.87      0.76       804\n",
            "     Current       0.90      0.73      0.81      1266\n",
            "\n",
            "    accuracy                           0.78      2070\n",
            "   macro avg       0.79      0.80      0.78      2070\n",
            "weighted avg       0.81      0.78      0.79      2070\n",
            "\n",
            "=== All Accuracy Scores ===\n",
            "[0.78881146 0.78881146 0.82791328 0.82055749 0.7578125  0.76875\n",
            " 0.7984375  0.715625   0.7671875  0.790625   0.81964286 0.74246032\n",
            " 0.87400794 0.7265873  0.80595238 0.86845238 0.8343254  0.83313492\n",
            " 0.7968254  0.8468254 ]\n",
            "\n",
            "\n",
            "=== Mean Accuracy Score ===\n",
            "Mean Accuracy Score - ON Text:  0.7986372737611306\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aSbG-17T43w"
      },
      "source": [
        "# Feature selection Filter Method ANOVA and k=50\n",
        "# Mean Accuracy Score - ON Text:  0.8764379476384049\n",
        "#Feature selection Filter Method ANOVA and k=10\n",
        "#Mean Accuracy Score - ON Text:  0.7982404483643052\n",
        "#Feature selection Filter Method ANOVA and k=15\n",
        "#Mean Accuracy Score - ON Text:  0.7986372737611306"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njS0GFrq4RrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "014f972d-edff-413e-9c29-02303f6bd554"
      },
      "source": [
        "# Split the data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(DF_TF_IDF_Filter50, y, test_size = 0.2, random_state = 1)\n",
        "\n",
        "#Construct a Gradient Boosting Classifier on combined data\n",
        "clf=GradientBoostingClassifier()\n",
        "RF_Comb = clf.fit(X_train,y_train)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(X_train, y_train)))\n",
        "rf_predictions = clf.predict(X_test)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_test, rf_predictions))\n",
        "\n",
        "#run cross-validation - COMBINED Data\n",
        "rf_Comb_cv_score = cross_val_score(RF_Comb, DF_TF_IDF, y, cv=20, scoring=\"balanced_accuracy\")\n",
        "print(\"=== All Accuracy Scores ===\")\n",
        "print(rf_Comb_cv_score)\n",
        "print('\\n')\n",
        "print(\"=== Mean Accuracy Score ===\")\n",
        "print(\"Mean Accuracy Score - ON Text: \",rf_Comb_cv_score.mean())\n",
        "print('\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score (training): 0.903986\n",
            "Confusion Matrix:\n",
            "[[129  21]\n",
            " [ 32 232]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.80      0.86      0.83       150\n",
            "     Current       0.92      0.88      0.90       264\n",
            "\n",
            "    accuracy                           0.87       414\n",
            "   macro avg       0.86      0.87      0.86       414\n",
            "weighted avg       0.88      0.87      0.87       414\n",
            "\n",
            "=== All Accuracy Scores ===\n",
            "[0.86817654 0.89566396 0.93960511 0.87127371 0.8625     0.91875\n",
            " 0.8515625  0.809375   0.8875     0.896875   0.94325397 0.88869048\n",
            " 0.92956349 0.74801587 0.88869048 0.90238095 0.89325397 0.90912698\n",
            " 0.84444444 0.92281746]\n",
            "\n",
            "\n",
            "=== Mean Accuracy Score ===\n",
            "Mean Accuracy Score - ON Text:  0.8835759956929928\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQYslfGmJiIg"
      },
      "source": [
        "DF_Combined1= pd.DataFrame(DF_TF_IDF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Mr7RFFl56GU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4fa5978-0234-46d6-83d6-04ede8726810"
      },
      "source": [
        "#Sequential Forward Search using Decision Tree Classifier\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "sfs1 = SFS(clf, \n",
        "           k_features=7, \n",
        "           forward=True, \n",
        "           floating=False, \n",
        "           verbose=2,\n",
        "           scoring='accuracy',\n",
        "           cv=2)\n",
        "\n",
        "sfs1 = sfs1.fit(DF_Combined1,y)\n",
        "sfs1.subsets_\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 377 out of 377 | elapsed:    3.5s finished\n",
            "\n",
            "[2020-10-30 23:25:54] Features: 1/7 -- score: 0.7932367149758455[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 376 out of 376 | elapsed:    4.1s finished\n",
            "\n",
            "[2020-10-30 23:25:58] Features: 2/7 -- score: 0.8217391304347826[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 375 out of 375 | elapsed:    5.1s finished\n",
            "\n",
            "[2020-10-30 23:26:03] Features: 3/7 -- score: 0.8415458937198068[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 374 out of 374 | elapsed:    6.6s finished\n",
            "\n",
            "[2020-10-30 23:26:09] Features: 4/7 -- score: 0.8478260869565217[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 373 out of 373 | elapsed:    4.2s finished\n",
            "\n",
            "[2020-10-30 23:26:14] Features: 5/7 -- score: 0.8555555555555555[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 372 out of 372 | elapsed:    4.2s finished\n",
            "\n",
            "[2020-10-30 23:26:18] Features: 6/7 -- score: 0.8579710144927536[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 371 out of 371 | elapsed:    6.6s finished\n",
            "\n",
            "[2020-10-30 23:26:24] Features: 7/7 -- score: 0.8589371980676328"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: {'avg_score': 0.7932367149758455,\n",
              "  'cv_scores': array([0.74202899, 0.84444444]),\n",
              "  'feature_idx': (3,),\n",
              "  'feature_names': ('Age',)},\n",
              " 2: {'avg_score': 0.8217391304347826,\n",
              "  'cv_scores': array([0.78164251, 0.86183575]),\n",
              "  'feature_idx': (3, 363),\n",
              "  'feature_names': ('Age', 'x0_F')},\n",
              " 3: {'avg_score': 0.8415458937198068,\n",
              "  'cv_scores': array([0.79516908, 0.88792271]),\n",
              "  'feature_idx': (3, 229, 363),\n",
              "  'feature_names': ('Age', 220, 'x0_F')},\n",
              " 4: {'avg_score': 0.8478260869565217,\n",
              "  'cv_scores': array([0.80676329, 0.88888889]),\n",
              "  'feature_idx': (3, 229, 233, 363),\n",
              "  'feature_names': ('Age', 220, 224, 'x0_F')},\n",
              " 5: {'avg_score': 0.8555555555555555,\n",
              "  'cv_scores': array([0.82318841, 0.88792271]),\n",
              "  'feature_idx': (3, 229, 233, 306, 363),\n",
              "  'feature_names': ('Age', 220, 224, 297, 'x0_F')},\n",
              " 6: {'avg_score': 0.8579710144927536,\n",
              "  'cv_scores': array([0.82705314, 0.88888889]),\n",
              "  'feature_idx': (3, 163, 229, 233, 306, 363),\n",
              "  'feature_names': ('Age', 154, 220, 224, 297, 'x0_F')},\n",
              " 7: {'avg_score': 0.8589371980676328,\n",
              "  'cv_scores': array([0.82898551, 0.88888889]),\n",
              "  'feature_idx': (3, 156, 163, 229, 233, 306, 363),\n",
              "  'feature_names': ('Age', 147, 154, 220, 224, 297, 'x0_F')}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qSxscpT8Jh4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36963e2b-9066-4faa-ac52-e59d1a9071f1"
      },
      "source": [
        "#Sequential forward search result\n",
        "print(sfs1.k_feature_names_)\n",
        "print(sfs1.k_score_)\n",
        "# Use features from Sequential forward Selection\n",
        "feat_cols = list(sfs1.k_feature_idx_)\n",
        "X_SFS=DF_Combined1.iloc[:, feat_cols]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Age', 147, 154, 220, 224, 297, 'x0_F')\n",
            "0.8589371980676328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U8QzV255hCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c2185a6-d683-4d8c-b721-6fdffaa5a461"
      },
      "source": [
        "# Split the data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_SFS, y, test_size = 0.2, random_state = 1)\n",
        "\n",
        "#Construct a Random Forest Classifier on combined data\n",
        "clf=RandomForestClassifier()\n",
        "RF_Comb = clf.fit(X_train,y_train)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(X_train, y_train)))\n",
        "rf_predictions = clf.predict(X_test)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_test, rf_predictions))\n",
        "\n",
        "#run cross-validation - COMBINED Data\n",
        "rf_Comb_cv_score = cross_val_score(RF_Comb, DF_TF_IDF, y, cv=20, scoring=\"balanced_accuracy\")\n",
        "print(\"=== All Accuracy Scores ===\")\n",
        "print(rf_Comb_cv_score)\n",
        "print('\\n')\n",
        "print(\"=== Mean Accuracy Score ===\")\n",
        "print(\"Mean Accuracy Score - ON Text: \",rf_Comb_cv_score.mean())\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score (training): 0.918478\n",
            "Confusion Matrix:\n",
            "[[129  21]\n",
            " [ 26 238]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.83      0.86      0.85       150\n",
            "     Current       0.92      0.90      0.91       264\n",
            "\n",
            "    accuracy                           0.89       414\n",
            "   macro avg       0.88      0.88      0.88       414\n",
            "weighted avg       0.89      0.89      0.89       414\n",
            "\n",
            "=== All Accuracy Scores ===\n",
            "[0.84436702 0.87611305 0.89508324 0.89140534 0.8953125  0.921875\n",
            " 0.8765625  0.821875   0.8796875  0.88125    0.91825397 0.90119048\n",
            " 0.93075397 0.7968254  0.90575397 0.86488095 0.91031746 0.93075397\n",
            " 0.90238095 0.91031746]\n",
            "\n",
            "\n",
            "=== Mean Accuracy Score ===\n",
            "Mean Accuracy Score - ON Text:  0.8877479856271778\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Rym7xSGKEp6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6aa59b7-97c0-43fa-9274-14d672395709"
      },
      "source": [
        "#Sequential Forward Search USing KNN Classifier\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "\n",
        "clf = KNeighborsClassifier()\n",
        "\n",
        "sfs1 = SFS(clf, \n",
        "           k_features=7, \n",
        "           forward=True, \n",
        "           floating=False, \n",
        "           verbose=2,\n",
        "           scoring='accuracy',\n",
        "           cv=2)\n",
        "\n",
        "sfs1 = sfs1.fit(DF_Combined1,y)\n",
        "sfs1.subsets_\n",
        "\n",
        "#Sequential forward search result\n",
        "print(sfs1.k_feature_names_)\n",
        "print(sfs1.k_score_)\n",
        "# Use features from Sequential forward Selection\n",
        "feat_cols = list(sfs1.k_feature_idx_)\n",
        "X_SFS=DF_Combined1.iloc[:, feat_cols]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 377 out of 377 | elapsed:   31.1s finished\n",
            "\n",
            "[2020-10-30 20:22:54] Features: 1/7 -- score: 0.8884057971014493[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 376 out of 376 | elapsed:   19.2s finished\n",
            "\n",
            "[2020-10-30 20:23:13] Features: 2/7 -- score: 0.8956521739130435[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 375 out of 375 | elapsed:   19.9s finished\n",
            "\n",
            "[2020-10-30 20:23:33] Features: 3/7 -- score: 0.8966183574879227[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 374 out of 374 | elapsed:   20.2s finished\n",
            "\n",
            "[2020-10-30 20:23:53] Features: 4/7 -- score: 0.8975845410628019[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 373 out of 373 | elapsed:   20.7s finished\n",
            "\n",
            "[2020-10-30 20:24:14] Features: 5/7 -- score: 0.8980676328502415[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 372 out of 372 | elapsed:   20.3s finished\n",
            "\n",
            "[2020-10-30 20:24:34] Features: 6/7 -- score: 0.8985507246376812[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "('Usage', 22, 27, 92, 110, 'x0_F', 'x0_M')\n",
            "0.8990338164251208\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done 371 out of 371 | elapsed:   20.7s finished\n",
            "\n",
            "[2020-10-30 20:24:55] Features: 7/7 -- score: 0.8990338164251208"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmoo3IWaKJAE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5bb034b-7236-4602-ba2e-162652cdd6ec"
      },
      "source": [
        "# Split the data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_SFS, y, test_size = 0.2, random_state = 1)\n",
        "\n",
        "#Construct a Gradient Boosting Classifier on combined data\n",
        "clf=GradientBoostingClassifier()\n",
        "RF_Comb = clf.fit(X_train,y_train)\n",
        "print(\"Accuracy score (training): {0:.6f}\".format(clf.score(X_train, y_train)))\n",
        "rf_predictions = clf.predict(X_test)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, rf_predictions))\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y_test, rf_predictions))\n",
        "\n",
        "#run cross-validation - COMBINED Data\n",
        "rf_Comb_cv_score = cross_val_score(RF_Comb, DF_TF_IDF, y, cv=20, scoring=\"balanced_accuracy\")\n",
        "print(\"=== All Accuracy Scores ===\")\n",
        "print(rf_Comb_cv_score)\n",
        "print('\\n')\n",
        "print(\"=== Mean Accuracy Score ===\")\n",
        "print(\"Mean Accuracy Score - ON Text: \",rf_Comb_cv_score.mean())\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score (training): 0.816425\n",
            "Confusion Matrix:\n",
            "[[ 94  56]\n",
            " [ 31 233]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Cancelled       0.75      0.63      0.68       150\n",
            "     Current       0.81      0.88      0.84       264\n",
            "\n",
            "    accuracy                           0.79       414\n",
            "   macro avg       0.78      0.75      0.76       414\n",
            "weighted avg       0.79      0.79      0.79       414\n",
            "\n",
            "=== All Accuracy Scores ===\n",
            "[0.86817654 0.89566396 0.93960511 0.87127371 0.8625     0.91875\n",
            " 0.8515625  0.809375   0.8875     0.896875   0.94325397 0.88869048\n",
            " 0.92956349 0.74801587 0.88869048 0.90238095 0.89325397 0.90912698\n",
            " 0.84444444 0.92281746]\n",
            "\n",
            "\n",
            "=== Mean Accuracy Score ===\n",
            "Mean Accuracy Score - ON Text:  0.8835759956929928\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}